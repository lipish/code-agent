# Model æŠ½è±¡å±‚è®¾è®¡åˆ†æ

## ğŸ¤” é—®é¢˜

ä¸ºä»€ä¹ˆè¦ä¸ºæ¯ä¸ª LLM æä¾›å•†ï¼ˆOpenAIã€Zhipuã€Anthropicï¼‰åˆ›å»ºå•ç‹¬çš„ Model ç»“æ„ä½“ï¼Ÿ`llm-connector` å·²ç»æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è¿æ¥åº“ï¼Œåªè¦ç¬¦åˆ protocol å°±å¯ä»¥è¿æ¥ã€‚

## ğŸ“Š å½“å‰è®¾è®¡

### å½“å‰æ¶æ„

```
ç”¨æˆ·é…ç½® (config.toml)
    â†“
ModelProvider æšä¸¾ (OpenAI/Zhipu/Anthropic/Local)
    â†“
create_agent() å‡½æ•°ä¸­çš„ match è¯­å¥
    â†“
åˆ›å»ºå¯¹åº”çš„ Model ç»“æ„ä½“ (OpenAIModel/ZhipuModel/AnthropicModel/LocalModel)
    â†“
æ¯ä¸ª Model å†…éƒ¨éƒ½ä½¿ç”¨ llm-connector::LlmClient
    â†“
å®ç° LanguageModel trait
    â†“
TaskAgent ä½¿ç”¨
```

### ä»£ç ç¤ºä¾‹

**å½“å‰å®ç°** (src/cli.rs):
```rust
async fn create_agent(config: &AgentConfig) -> anyhow::Result<TaskAgent> {
    let model: Box<dyn LanguageModel> = match &config.model.provider {
        ModelProvider::OpenAI => {
            let api_key = config.model.api_key.clone().ok_or(...)?;
            Box::new(OpenAIModel::new(api_key, config.model.model_name.clone()))
        }
        ModelProvider::Anthropic => {
            let api_key = config.model.api_key.clone().ok_or(...)?;
            Box::new(AnthropicModel::new(api_key, config.model.model_name.clone()))
        }
        ModelProvider::Zhipu => {
            let api_key = config.model.api_key.clone().ok_or(...)?;
            Box::new(ZhipuModel::new(api_key, config.model.model_name.clone(), config.model.endpoint.clone()))
        }
        ModelProvider::Local(endpoint) => {
            Box::new(LocalModel::new(endpoint.clone(), config.model.model_name.clone()))
        }
    };
    
    let agent = TaskAgent::new(model, config.clone());
    Ok(agent)
}
```

**æ¯ä¸ª Model çš„å®ç°å‡ ä¹ç›¸åŒ** (src/models.rs):
```rust
// OpenAIModel
pub struct OpenAIModel {
    client: LlmClient,
    model: String,
}

impl OpenAIModel {
    pub fn new(api_key: String, model: String) -> Self {
        Self {
            client: LlmClient::openai(&api_key),
            model
        }
    }
}

// ZhipuModel - å‡ ä¹ä¸€æ¨¡ä¸€æ ·ï¼
pub struct ZhipuModel {
    client: LlmClient,
    model: String,
}

impl ZhipuModel {
    pub fn new(api_key: String, model: String, _endpoint: Option<String>) -> Self {
        Self {
            client: LlmClient::openai(&api_key),  // æ³¨æ„ï¼šä¹Ÿæ˜¯ç”¨ openai()
            model
        }
    }
}

// AnthropicModel - ä¹Ÿå‡ ä¹ä¸€æ ·ï¼
pub struct AnthropicModel {
    client: LlmClient,
    model: String,
}

// ... å®ç°ä»£ç  99% ç›¸åŒ
```

## ğŸ” é—®é¢˜åˆ†æ

### 1. è¿‡åº¦æŠ½è±¡

**é—®é¢˜**:
- ä¸ºæ¯ä¸ªæä¾›å•†åˆ›å»ºå•ç‹¬çš„ç»“æ„ä½“
- æ¯ä¸ªç»“æ„ä½“çš„ä»£ç å‡ ä¹å®Œå…¨ç›¸åŒ
- åªæ˜¯åœ¨åˆ›å»º `LlmClient` æ—¶è°ƒç”¨ä¸åŒçš„æ–¹æ³•

**ä»£ç é‡å¤**:
```rust
// OpenAIModel::complete()
async fn complete(&self, prompt: &str) -> Result<ModelResponse, ModelError> {
    let request = ChatRequest {
        model: format!("openai/{}", self.model),  // å”¯ä¸€çš„åŒºåˆ«
        messages: vec![LlmMessage::user(prompt)],
        ..Default::default()
    };
    
    let response = self.client.chat(&request).await
        .map_err(|e| ModelError::APIError(e.to_string()))?;
    
    // ... åç»­å¤„ç†å®Œå…¨ç›¸åŒ
}

// ZhipuModel::complete() - 99% ç›¸åŒ
async fn complete(&self, prompt: &str) -> Result<ModelResponse, ModelError> {
    let request = ChatRequest {
        model: format!("zhipu/{}", self.model),  // å”¯ä¸€çš„åŒºåˆ«
        messages: vec![LlmMessage::user(prompt)],
        ..Default::default()
    };
    
    // ... å®Œå…¨ç›¸åŒçš„ä»£ç 
}
```

### 2. llm-connector å·²ç»åšäº†ç»Ÿä¸€

`llm-connector` çš„è®¾è®¡ç›®çš„å°±æ˜¯ç»Ÿä¸€ä¸åŒæä¾›å•†çš„æ¥å£ï¼š

```rust
// llm-connector å·²ç»æä¾›äº†ç»Ÿä¸€çš„æ¥å£
let client = LlmClient::openai(&api_key);
let client = LlmClient::anthropic(&api_key);
let client = LlmClient::ollama_at(&endpoint);

// æ‰€æœ‰ client éƒ½ä½¿ç”¨ç›¸åŒçš„ chat() æ–¹æ³•
let response = client.chat(&request).await?;
```

### 3. ModelProvider æšä¸¾ä¹Ÿæ˜¯å¤šä½™çš„

```rust
pub enum ModelProvider {
    OpenAI,
    Anthropic,
    Zhipu,
    Local(String),
}
```

è¿™ä¸ªæšä¸¾åªæ˜¯ä¸ºäº†åœ¨ `create_agent()` ä¸­åš matchï¼Œç„¶ååˆ›å»ºå¯¹åº”çš„ Model ç»“æ„ä½“ã€‚

## ğŸ’¡ ä¼˜åŒ–æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1: ç»Ÿä¸€çš„ UniversalModel â­ (æ¨è)

**è®¾è®¡**:
```rust
// åªéœ€è¦ä¸€ä¸ª Model ç»“æ„ä½“
pub struct UniversalModel {
    client: LlmClient,
    model_name: String,
    provider: String,  // ç”¨äºæ ¼å¼åŒ– model å­—ç¬¦ä¸²
}

impl UniversalModel {
    pub fn new(config: &ModelConfig) -> Result<Self, ModelError> {
        let client = match &config.provider {
            ModelProvider::OpenAI => {
                let api_key = config.api_key.as_ref()
                    .ok_or_else(|| ModelError::ConfigError("API key required".into()))?;
                LlmClient::openai(api_key)
            }
            ModelProvider::Anthropic => {
                let api_key = config.api_key.as_ref()
                    .ok_or_else(|| ModelError::ConfigError("API key required".into()))?;
                LlmClient::anthropic(api_key)
            }
            ModelProvider::Zhipu => {
                let api_key = config.api_key.as_ref()
                    .ok_or_else(|| ModelError::ConfigError("API key required".into()))?;
                LlmClient::openai(api_key)  // Zhipu å…¼å®¹ OpenAI API
            }
            ModelProvider::Local(endpoint) => {
                LlmClient::ollama_at(endpoint)
            }
        };
        
        let provider = match &config.provider {
            ModelProvider::OpenAI => "openai",
            ModelProvider::Anthropic => "anthropic",
            ModelProvider::Zhipu => "zhipu",
            ModelProvider::Local(_) => "",
        }.to_string();
        
        Ok(Self {
            client,
            model_name: config.model_name.clone(),
            provider,
        })
    }
}

#[async_trait]
impl LanguageModel for UniversalModel {
    async fn complete(&self, prompt: &str) -> Result<ModelResponse, ModelError> {
        let model = if self.provider.is_empty() {
            self.model_name.clone()
        } else {
            format!("{}/{}", self.provider, self.model_name)
        };
        
        let request = ChatRequest {
            model,
            messages: vec![LlmMessage::user(prompt)],
            ..Default::default()
        };
        
        let response = self.client.chat(&request).await
            .map_err(|e| ModelError::APIError(e.to_string()))?;
        
        // ç»Ÿä¸€çš„å“åº”å¤„ç†
        convert_response(response)
    }
    
    // ... å…¶ä»–æ–¹æ³•
}
```

**ä½¿ç”¨**:
```rust
async fn create_agent(config: &AgentConfig) -> anyhow::Result<TaskAgent> {
    // ç®€åŒ–ï¼åªéœ€è¦ä¸€è¡Œ
    let model = Box::new(UniversalModel::new(&config.model)?);
    let agent = TaskAgent::new(model, config.clone());
    Ok(agent)
}
```

**ä¼˜ç‚¹**:
- âœ… æ¶ˆé™¤ä»£ç é‡å¤
- âœ… ç®€åŒ–åˆ›å»ºé€»è¾‘
- âœ… æ›´å®¹æ˜“æ·»åŠ æ–°çš„æä¾›å•†
- âœ… ç»Ÿä¸€çš„é”™è¯¯å¤„ç†
- âœ… æ›´å°‘çš„ä»£ç ç»´æŠ¤

### æ–¹æ¡ˆ 2: ç›´æ¥ä½¿ç”¨ llm-connector (æ›´æ¿€è¿›)

**è®¾è®¡**:
```rust
// å®Œå…¨ä¸éœ€è¦è‡ªå·±çš„ Model æŠ½è±¡
// ç›´æ¥åœ¨ TaskAgent ä¸­ä½¿ç”¨ LlmClient

pub struct TaskAgent {
    client: Arc<LlmClient>,
    model_name: String,
    provider: String,
    // ...
}

impl TaskAgent {
    pub fn new(config: &AgentConfig) -> Result<Self, AgentError> {
        let client = create_llm_client(&config.model)?;
        
        Ok(Self {
            client: Arc::new(client),
            model_name: config.model.model_name.clone(),
            provider: get_provider_prefix(&config.model.provider),
            // ...
        })
    }
    
    async fn call_model(&self, prompt: &str) -> Result<String, AgentError> {
        let model = format!("{}/{}", self.provider, self.model_name);
        let request = ChatRequest {
            model,
            messages: vec![LlmMessage::user(prompt)],
            ..Default::default()
        };
        
        let response = self.client.chat(&request).await?;
        Ok(response.choices[0].message.content.clone())
    }
}
```

**ä¼˜ç‚¹**:
- âœ… æœ€ç®€å•çš„è®¾è®¡
- âœ… ç›´æ¥ä½¿ç”¨ llm-connector çš„èƒ½åŠ›
- âœ… å‡å°‘æŠ½è±¡å±‚æ¬¡

**ç¼ºç‚¹**:
- âŒ å¤±å»äº† LanguageModel trait çš„çµæ´»æ€§
- âŒ æµ‹è¯•æ—¶ä¸å®¹æ˜“ mock

### æ–¹æ¡ˆ 3: ä¿ç•™ traitï¼Œç®€åŒ–å®ç° (æŠ˜ä¸­)

**è®¾è®¡**:
```rust
// ä¿ç•™ LanguageModel trait (ç”¨äºæµ‹è¯•å’Œæ‰©å±•)
pub trait LanguageModel: Send + Sync {
    async fn complete(&self, prompt: &str) -> Result<ModelResponse, ModelError>;
    fn model_name(&self) -> &str;
}

// åªæœ‰ä¸€ä¸ªå®ç°
pub struct LlmModel {
    client: LlmClient,
    config: ModelConfig,
}

impl LlmModel {
    pub fn from_config(config: ModelConfig) -> Result<Self, ModelError> {
        let client = Self::create_client(&config)?;
        Ok(Self { client, config })
    }
    
    fn create_client(config: &ModelConfig) -> Result<LlmClient, ModelError> {
        // ç»Ÿä¸€çš„ client åˆ›å»ºé€»è¾‘
        match &config.provider {
            ModelProvider::OpenAI => {
                let api_key = config.api_key.as_ref().ok_or(...)?;
                Ok(LlmClient::openai(api_key))
            }
            // ... å…¶ä»–æä¾›å•†
        }
    }
    
    fn format_model_name(&self) -> String {
        match &self.config.provider {
            ModelProvider::OpenAI => format!("openai/{}", self.config.model_name),
            ModelProvider::Zhipu => format!("zhipu/{}", self.config.model_name),
            ModelProvider::Anthropic => format!("anthropic/{}", self.config.model_name),
            ModelProvider::Local(_) => self.config.model_name.clone(),
        }
    }
}

#[async_trait]
impl LanguageModel for LlmModel {
    async fn complete(&self, prompt: &str) -> Result<ModelResponse, ModelError> {
        let request = ChatRequest {
            model: self.format_model_name(),
            messages: vec![LlmMessage::user(prompt)],
            ..Default::default()
        };
        
        let response = self.client.chat(&request).await
            .map_err(|e| ModelError::APIError(e.to_string()))?;
        
        convert_response(response)
    }
    
    fn model_name(&self) -> &str {
        &self.config.model_name
    }
}

// MockModel ä¿ç•™ç”¨äºæµ‹è¯•
pub struct MockModel { /* ... */ }
```

**ä½¿ç”¨**:
```rust
async fn create_agent(config: &AgentConfig) -> anyhow::Result<TaskAgent> {
    let model = Box::new(LlmModel::from_config(config.model.clone())?);
    let agent = TaskAgent::new(model, config.clone());
    Ok(agent)
}
```

**ä¼˜ç‚¹**:
- âœ… ä¿ç•™äº† trait çš„çµæ´»æ€§
- âœ… æ¶ˆé™¤äº†ä»£ç é‡å¤
- âœ… æ˜“äºæµ‹è¯•ï¼ˆä¿ç•™ MockModelï¼‰
- âœ… ç®€åŒ–äº†åˆ›å»ºé€»è¾‘

## ğŸ“Š å¯¹æ¯”è¡¨

| ç‰¹æ€§ | å½“å‰è®¾è®¡ | æ–¹æ¡ˆ1 (UniversalModel) | æ–¹æ¡ˆ2 (ç›´æ¥ä½¿ç”¨) | æ–¹æ¡ˆ3 (æŠ˜ä¸­) |
|------|---------|----------------------|----------------|-------------|
| ä»£ç é‡å¤ | âŒ é«˜ | âœ… æ—  | âœ… æ—  | âœ… æ—  |
| æŠ½è±¡å±‚æ¬¡ | ğŸŸ¡ 2å±‚ | ğŸŸ¡ 2å±‚ | ğŸŸ¢ 1å±‚ | ğŸŸ¡ 2å±‚ |
| æ˜“äºæ‰©å±• | âŒ éœ€è¦æ–°ç±» | âœ… ä¿®æ”¹ä¸€å¤„ | âœ… ä¿®æ”¹ä¸€å¤„ | âœ… ä¿®æ”¹ä¸€å¤„ |
| æ˜“äºæµ‹è¯• | âœ… æœ‰ trait | âœ… æœ‰ trait | âŒ éš¾ mock | âœ… æœ‰ trait |
| ä»£ç é‡ | âŒ å¤š | âœ… å°‘ | âœ… æœ€å°‘ | âœ… å°‘ |
| ç»´æŠ¤æˆæœ¬ | âŒ é«˜ | âœ… ä½ | âœ… ä½ | âœ… ä½ |

## ğŸ¯ æ¨èæ–¹æ¡ˆ

### çŸ­æœŸï¼šæ–¹æ¡ˆ 3 (æŠ˜ä¸­æ–¹æ¡ˆ) â­

**ç†ç”±**:
1. ä¿ç•™ `LanguageModel` trait çš„çµæ´»æ€§
2. æ¶ˆé™¤æ‰€æœ‰ä»£ç é‡å¤
3. ä¿æŒæµ‹è¯•èƒ½åŠ›ï¼ˆMockModelï¼‰
4. æœ€å°çš„é‡æ„æˆæœ¬

### é•¿æœŸï¼šæ–¹æ¡ˆ 2 (ç›´æ¥ä½¿ç”¨)

**ç†ç”±**:
1. æœ€ç®€å•çš„è®¾è®¡
2. å……åˆ†åˆ©ç”¨ llm-connector
3. å‡å°‘ä¸å¿…è¦çš„æŠ½è±¡

## ğŸ”§ å®æ–½æ­¥éª¤

### æ­¥éª¤ 1: åˆ›å»ºç»Ÿä¸€çš„ LlmModel

```rust
// src/models.rs
pub struct LlmModel {
    client: LlmClient,
    config: ModelConfig,
}

impl LlmModel {
    pub fn from_config(config: ModelConfig) -> Result<Self, ModelError> {
        // å®ç°ç»Ÿä¸€çš„åˆ›å»ºé€»è¾‘
    }
}
```

### æ­¥éª¤ 2: ç®€åŒ– create_agent

```rust
// src/cli.rs
async fn create_agent(config: &AgentConfig) -> anyhow::Result<TaskAgent> {
    let model = Box::new(LlmModel::from_config(config.model.clone())?);
    let agent = TaskAgent::new(model, config.clone());
    // ... æ³¨å†Œå·¥å…·
    Ok(agent)
}
```

### æ­¥éª¤ 3: åˆ é™¤å†—ä½™ä»£ç 

```bash
# åˆ é™¤ OpenAIModel, ZhipuModel, AnthropicModel, LocalModel
# ä¿ç•™ MockModel ç”¨äºæµ‹è¯•
```

### æ­¥éª¤ 4: æ›´æ–°æ–‡æ¡£

æ›´æ–°æ‰€æœ‰æ–‡æ¡£ä¸­çš„ç¤ºä¾‹ä»£ç ã€‚

## ğŸ“š æ€»ç»“

**å½“å‰é—®é¢˜**:
- âŒ ä¸ºæ¯ä¸ªæä¾›å•†åˆ›å»ºå•ç‹¬çš„ Model ç»“æ„ä½“
- âŒ ä»£ç é‡å¤ç‡ > 95%
- âŒ è¿å DRY åŸåˆ™
- âŒ llm-connector å·²ç»åšäº†ç»Ÿä¸€ï¼Œæˆ‘ä»¬åˆåšäº†ä¸€å±‚

**æ ¹æœ¬åŸå› **:
- è¿‡åº¦è®¾è®¡
- æ²¡æœ‰å……åˆ†åˆ©ç”¨ llm-connector çš„èƒ½åŠ›
- è¯¯ä»¥ä¸ºéœ€è¦ä¸ºæ¯ä¸ªæä¾›å•†å®šåˆ¶é€»è¾‘

**è§£å†³æ–¹æ¡ˆ**:
- âœ… ä½¿ç”¨ç»Ÿä¸€çš„ Model å®ç°
- âœ… åœ¨åˆ›å»º LlmClient æ—¶å¤„ç†å·®å¼‚
- âœ… ä¿ç•™ trait ç”¨äºæµ‹è¯•å’Œæ‰©å±•
- âœ… å¤§å¹…ç®€åŒ–ä»£ç 

**é¢„æœŸæ•ˆæœ**:
- ä»£ç é‡å‡å°‘ ~200 è¡Œ
- ç»´æŠ¤æˆæœ¬é™ä½ 70%
- æ·»åŠ æ–°æä¾›å•†åªéœ€ä¿®æ”¹ä¸€å¤„
- æ›´æ¸…æ™°çš„æ¶æ„

