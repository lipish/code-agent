use tokio;
use task_runner::models::{LlmModel, LanguageModel};
use task_runner::config::{ModelConfig, ModelProvider};

#[tokio::test]
async fn test_raw_llm_interaction_observation() -> Result<(), Box<dyn std::error::Error>> {
    println!("\nğŸ” ç›´æ¥è§‚å¯ŸLLMæ¨¡å‹äº¤äº’è¿‡ç¨‹");
    println!("=====================================");
    
    // åˆ›å»ºæ¨¡å‹é…ç½®
    let config = ModelConfig {
        provider: ModelProvider::Zhipu,
        model_name: "glm-4-flash".to_string(),
        api_key: Some(std::env::var("ZHIPU_API_KEY").unwrap_or_else(|_| "your-api-key-here".to_string())),
        endpoint: Some("https://open.bigmodel.cn/api/paas/v4".to_string()),
        max_tokens: 2000,
        temperature: 0.7,
    };
    
    println!("ğŸ“‹ æ¨¡å‹é…ç½®:");
    println!("  - æä¾›å•†: {:?}", config.provider);
    println!("  - æ¨¡å‹: {}", config.model_name);
    println!("  - æœ€å¤§ä»¤ç‰Œ: {}", config.max_tokens);
    println!("  - æ¸©åº¦: {}", config.temperature);
    
    // åˆ›å»ºLLMå®ä¾‹
    let llm = match LlmModel::from_config(config) {
        Ok(model) => model,
        Err(e) => {
            println!("âŒ LLMå®ä¾‹åˆ›å»ºå¤±è´¥: {}", e);
            return Err(Box::new(e) as Box<dyn std::error::Error>);
        }
    };
    println!("âœ… LLMå®ä¾‹åˆ›å»ºæˆåŠŸ");
    
    // æ„é€ è¾“å…¥æç¤ºè¯
    let input_prompt = r#"ä½ æ˜¯ä¸€ä¸ªä»»åŠ¡è§„åˆ’ä¸“å®¶ã€‚è¯·å°†ä»¥ä¸‹ä»»åŠ¡åˆ†è§£ä¸ºè¯¦ç»†çš„æ­¥éª¤è®¡åˆ’ï¼š

ä»»åŠ¡ï¼šåˆ›å»ºä¸€ä¸ªRustå·¥ç¨‹ï¼Œåå­—å«hello-world

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¿”å›ï¼š
1. ä»»åŠ¡ç†è§£ï¼š[æè¿°ä½ å¯¹ä»»åŠ¡çš„ç†è§£]
2. å‡†å¤‡å·¥ä½œï¼š[åˆ—å‡ºéœ€è¦çš„å‡†å¤‡å·¥ä½œ]
3. è¯¦ç»†æ­¥éª¤ï¼š[åˆ—å‡ºå…·ä½“çš„æ‰§è¡Œæ­¥éª¤]
4. éªŒè¯æ–¹æ³•ï¼š[å¦‚ä½•éªŒè¯ä»»åŠ¡å®Œæˆ]

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œå¹¶æä¾›è¯¦ç»†çš„æŠ€æœ¯è¯´æ˜ã€‚"#;
    
    println!("\nğŸ“ å‘é€ç»™æ¨¡å‹çš„æç¤ºè¯:");
    println!("=====================================");
    println!("{}", input_prompt);
    
    println!("\nğŸš€ æ­£åœ¨è°ƒç”¨LLMæ¨¡å‹...");
    println!("=====================================");
    
    let start_time = std::time::Instant::now();
    
    // ç›´æ¥è°ƒç”¨LLM
    match llm.complete(&input_prompt).await {
        Ok(response) => {
            let duration = start_time.elapsed();
            
            println!("âœ… LLMè°ƒç”¨æˆåŠŸï¼è€—æ—¶: {:?}", duration);
            println!("\nğŸ¤– æ¨¡å‹åŸå§‹è¿”å›å†…å®¹:");
            println!("=====================================");
            println!("{}", response.content);
            println!("=====================================");
            
            // åˆ†æè¿”å›å†…å®¹
            println!("\nğŸ“Š è¿”å›å†…å®¹åˆ†æ:");
            println!("  - å“åº”é•¿åº¦: {} å­—ç¬¦", response.content.len());
            println!("  - åŒ…å«è¡Œæ•°: {} è¡Œ", response.content.lines().count());
            
            // æ£€æŸ¥æ˜¯å¦åŒ…å«é¢„æœŸçš„ç»“æ„
            let has_task_understanding = response.content.contains("ä»»åŠ¡ç†è§£") || response.content.contains("ç†è§£");
            let has_preparation = response.content.contains("å‡†å¤‡å·¥ä½œ") || response.content.contains("å‡†å¤‡");
            let has_steps = response.content.contains("è¯¦ç»†æ­¥éª¤") || response.content.contains("æ­¥éª¤");
            let has_verification = response.content.contains("éªŒè¯") || response.content.contains("æ£€æŸ¥");
            
            println!("  - åŒ…å«ä»»åŠ¡ç†è§£: {}", if has_task_understanding { "âœ…" } else { "âŒ" });
            println!("  - åŒ…å«å‡†å¤‡å·¥ä½œ: {}", if has_preparation { "âœ…" } else { "âŒ" });
            println!("  - åŒ…å«è¯¦ç»†æ­¥éª¤: {}", if has_steps { "âœ…" } else { "âŒ" });
            println!("  - åŒ…å«éªŒè¯æ–¹æ³•: {}", if has_verification { "âœ…" } else { "âŒ" });
            
            // ç»Ÿè®¡æ•°å­—åˆ—è¡¨é¡¹
            let numbered_items = response.content.lines()
                .filter(|line| line.trim().chars().next().map_or(false, |c| c.is_ascii_digit()))
                .count();
            println!("  - ç¼–å·é¡¹ç›®æ•°: {}", numbered_items);
            
            // æ£€æŸ¥Rustç›¸å…³å†…å®¹
            let rust_mentions = response.content.matches("Rust").count() + 
                              response.content.matches("rust").count() + 
                              response.content.matches("cargo").count() +
                              response.content.matches("Cargo").count();
            println!("  - Rustç›¸å…³æåŠ: {} æ¬¡", rust_mentions);
            
            // æ˜¾ç¤ºtokenä½¿ç”¨æƒ…å†µ
            if let Some(usage) = &response.usage {
                println!("  - Tokenä½¿ç”¨: {} (è¾“å…¥: {}, è¾“å‡º: {})", 
                        usage.total_tokens, usage.prompt_tokens, usage.completion_tokens);
            }
            
            Ok(())
        },
        Err(e) => {
            let duration = start_time.elapsed();
            println!("âŒ LLMè°ƒç”¨å¤±è´¥ï¼è€—æ—¶: {:?}", duration);
            println!("é”™è¯¯ä¿¡æ¯: {}", e);
            Err(Box::new(e) as Box<dyn std::error::Error>)
        }
    }
}

#[tokio::test]
async fn test_step_by_step_observation() -> Result<(), Box<dyn std::error::Error>> {
    println!("\nğŸ” åˆ†æ­¥éª¤è§‚å¯ŸLLMäº¤äº’");
    println!("========================");
    
    let config = ModelConfig {
        provider: ModelProvider::Zhipu,
        model_name: "glm-4-flash".to_string(),
        api_key: Some(std::env::var("ZHIPU_API_KEY").unwrap_or_else(|_| "your-api-key-here".to_string())),
        endpoint: Some("https://open.bigmodel.cn/api/paas/v4".to_string()),
        max_tokens: 1500,
        temperature: 0.3,
    };
    
    let llm = match LlmModel::from_config(config) {
        Ok(model) => model,
        Err(e) => {
            println!("âŒ LLMå®ä¾‹åˆ›å»ºå¤±è´¥: {}", e);
            return Err(Box::new(e) as Box<dyn std::error::Error>);
        }
    };
    
    // ç¬¬ä¸€æ­¥ï¼šç†è§£ä»»åŠ¡
    let step1_prompt = "è¯·åˆ†æè¿™ä¸ªä»»åŠ¡ï¼šåˆ›å»ºä¸€ä¸ªRustå·¥ç¨‹ï¼Œåå­—å«hello-worldã€‚ä½ è®¤ä¸ºè¿™ä¸ªä»»åŠ¡çš„æ ¸å¿ƒè¦æ±‚æ˜¯ä»€ä¹ˆï¼Ÿ";
    
    println!("\nğŸ“ æ­¥éª¤1 - ä»»åŠ¡ç†è§£");
    println!("æç¤ºè¯: {}", step1_prompt);
    
    if let Ok(response1) = llm.complete(&step1_prompt).await {
        println!("ğŸ¤– æ¨¡å‹å›å¤:");
        println!("{}", response1.content);
        
        // ç¬¬äºŒæ­¥ï¼šåˆ¶å®šè®¡åˆ’
        let step2_prompt = format!(
            "åŸºäºä½ å¯¹ä»»åŠ¡çš„ç†è§£ï¼š{}ï¼Œç°åœ¨è¯·åˆ¶å®šè¯¦ç»†çš„æ‰§è¡Œè®¡åˆ’ï¼ŒåŒ…æ‹¬å…·ä½“çš„å‘½ä»¤å’Œæ­¥éª¤ã€‚", 
            response1.content.chars().take(200).collect::<String>()
        );
        
        println!("\nğŸ“ æ­¥éª¤2 - åˆ¶å®šè®¡åˆ’");
        println!("æç¤ºè¯: {}", step2_prompt);
        
        if let Ok(response2) = llm.complete(&step2_prompt).await {
            println!("ğŸ¤– æ¨¡å‹å›å¤:");
            println!("{}", response2.content);
            
            println!("\nâœ… ä¸¤æ­¥éª¤äº¤äº’å®Œæˆ");
        }
    }
    
    Ok(())
}