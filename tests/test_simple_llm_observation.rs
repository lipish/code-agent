//! ç®€åŒ–çš„LLMè§‚å¯Ÿæµ‹è¯•ï¼Œç›´æ¥æ˜¾ç¤ºè¾“å…¥è¾“å‡ºè¿‡ç¨‹

use tokio;
use task_runner::models::{LlmModel, LanguageModel};
use task_runner::config::{ModelConfig, ModelProvider};

#[tokio::test]
async fn test_simple_llm_input_output() -> Result<(), Box<dyn std::error::Error>> {
    println!("\nğŸ¯ ç®€åŒ–LLMè¾“å…¥è¾“å‡ºè§‚å¯Ÿ");
    println!("====================");
    
    // åˆ›å»ºæ¨¡å‹é…ç½®
    let config = ModelConfig {
        provider: ModelProvider::Zhipu,
        model_name: "glm-4-flash".to_string(),
        api_key: Some("your-api-key-here".to_string()),
        endpoint: Some("https://open.bigmodel.cn/api/paas/v4".to_string()),
        max_tokens: 800,
        temperature: 0.5,
    };
    
    println!("ğŸ”§ æ¨¡å‹é…ç½®å®Œæˆ: {} ({:?})", config.model_name, config.provider);
    
    // åˆ›å»ºLLMå®ä¾‹
    let llm = LlmModel::from_config(config)?;
    println!("âœ… LLMå®ä¾‹åˆ›å»ºæˆåŠŸ");
    
    // è¾“å…¥å†…å®¹
    let input = "ä»»åŠ¡ï¼šåˆ›å»ºä¸€ä¸ªRustå·¥ç¨‹ï¼Œåå­—å«hello-world\n\nè¯·ç”¨ä¸­æ–‡å›ç­”ï¼š\n1. è¿™ä¸ªä»»åŠ¡éœ€è¦å‡ ä¸ªæ­¥éª¤ï¼Ÿ\n2. ç¬¬ä¸€æ­¥åº”è¯¥åšä»€ä¹ˆï¼Ÿ\n3. æœ€é‡è¦çš„å‘½ä»¤æ˜¯ä»€ä¹ˆï¼Ÿ";
    
    println!("\nğŸ“¤ å‘é€ç»™LLMçš„è¾“å…¥:");
    println!("{}", "â”€".repeat(50));
    println!("{}", input);
    println!("{}", "â”€".repeat(50));
    
    println!("\nâ±ï¸ æ­£åœ¨ç­‰å¾…LLMå“åº”...");
    
    let start = std::time::Instant::now();
    match llm.complete(input).await {
        Ok(response) => {
            let duration = start.elapsed();
            
            println!("âœ… æ”¶åˆ°å“åº”ï¼è€—æ—¶: {:?}", duration);
            println!("\nğŸ“¥ LLMå®Œæ•´è¾“å‡º:");
            println!("{}", "â”€".repeat(50));
            println!("{}", response.content);
            println!("{}", "â”€".repeat(50));
            
            // ç®€å•ç»Ÿè®¡
            println!("\nğŸ“Š è¾“å‡ºç»Ÿè®¡:");
            println!("  - å­—ç¬¦æ•°: {}", response.content.len());
            println!("  - è¡Œæ•°: {}", response.content.lines().count());
            println!("  - åŒ…å«'cargo': {}", response.content.contains("cargo"));
            println!("  - åŒ…å«'rust': {}", response.content.to_lowercase().contains("rust"));
            
            if let Some(usage) = response.usage {
                println!("  - è¾“å…¥token: {}", usage.prompt_tokens);
                println!("  - è¾“å‡ºtoken: {}", usage.completion_tokens);
                println!("  - æ€»token: {}", usage.total_tokens);
            }
            
            Ok(())
        }
        Err(e) => {
            println!("âŒ LLMè°ƒç”¨å¤±è´¥: {}", e);
            Err(Box::new(e) as Box<dyn std::error::Error>)
        }
    }
}

#[tokio::test]
async fn test_minimal_connectivity() -> Result<(), Box<dyn std::error::Error>> {
    println!("\nğŸ”— æœ€å°è¿æ¥æµ‹è¯•");
    println!("===============");
    
    let config = ModelConfig {
        provider: ModelProvider::Zhipu,
        model_name: "glm-4-flash".to_string(),
        api_key: Some("your-api-key-here".to_string()),
        endpoint: Some("https://open.bigmodel.cn/api/paas/v4".to_string()),
        max_tokens: 100,
        temperature: 0.1,
    };
    
    let llm = LlmModel::from_config(config)?;
    
    let simple_input = "ç”¨ä¸­æ–‡å›ç­”ï¼šåˆ›å»ºRusté¡¹ç›®çš„ç¬¬ä¸€æ­¥æ˜¯ä»€ä¹ˆï¼Ÿ";
    println!("ğŸ“¤ è¾“å…¥: {}", simple_input);
    
    match llm.complete(simple_input).await {
        Ok(response) => {
            println!("ğŸ“¥ è¾“å‡º: {}", response.content.trim());
            println!("âœ… è¿æ¥æµ‹è¯•æˆåŠŸ");
            Ok(())
        }
        Err(e) => {
            println!("âŒ è¿æ¥æµ‹è¯•å¤±è´¥: {}", e);
            Err(Box::new(e) as Box<dyn std::error::Error>)
        }
    }
}