//! Demonstration of llm-connector 0.3.8 Features
//!
//! This example showcases the capabilities of llm-connector library:
//! - Multiple protocol support (OpenAI, Anthropic, Zhipu, Aliyun, Ollama)
//! - Model discovery (fetch available models from API)
//! - Unified interface across different providers
//! - Protocol information retrieval

use agent_runner::config::{ModelConfig, ModelProvider};
use agent_runner::models::{LlmModel, LanguageModel};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("ğŸš€ llm-connector 0.3.8 åŠŸèƒ½æ¼”ç¤º");
    println!("================================================================================\n");

    // Demo 1: OpenAI
    demo_openai().await?;
    
    // Demo 2: Zhipu (ä½¿ç”¨ä¸“ç”¨æ„é€ å‡½æ•°)
    demo_zhipu().await?;
    
    // Demo 3: Aliyun (ä½¿ç”¨ä¸“ç”¨æ„é€ å‡½æ•°)
    demo_aliyun().await?;
    
    // Demo 4: Ollama (æœ¬åœ°æ¨¡å‹)
    demo_ollama().await?;
    
    // Demo 5: OpenAI-compatible providers
    demo_openai_compatible().await?;

    Ok(())
}

async fn demo_openai() -> Result<(), Box<dyn std::error::Error>> {
    println!("ğŸ“– Demo 1: OpenAI Protocol");
    println!("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€");
    
    // æ£€æŸ¥ç¯å¢ƒå˜é‡
    if let Ok(api_key) = std::env::var("OPENAI_API_KEY") {
        let config = ModelConfig {
            provider: ModelProvider::OpenAI,
            model_name: "gpt-4".to_string(),
            api_key: Some(api_key),
            endpoint: None, // ä½¿ç”¨é»˜è®¤ç«¯ç‚¹
            max_tokens: 100,
            temperature: 0.7,
        };
        
        let model = LlmModel::from_config(config)?;
        
        println!("âœ“ åè®®: {}", model.protocol_name());
        println!("âœ“ æ¨¡å‹åç§°: {}", model.model_name());
        println!("âœ“ æ”¯æŒå·¥å…·: {}", if model.supports_tools() { "æ˜¯" } else { "å¦" });
        
        // å°è¯•è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
        match model.fetch_available_models().await {
            Ok(models) => {
                println!("âœ“ å¯ç”¨æ¨¡å‹: {:?}", &models[..models.len().min(5)]);
                if models.len() > 5 {
                    println!("  ... è¿˜æœ‰ {} ä¸ªæ¨¡å‹", models.len() - 5);
                }
            }
            Err(e) => println!("âš ï¸  è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {}", e),
        }
    } else {
        println!("âš ï¸  æœªè®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡ï¼Œè·³è¿‡æ­¤æ¼”ç¤º");
    }
    
    println!();
    Ok(())
}

async fn demo_zhipu() -> Result<(), Box<dyn std::error::Error>> {
    println!("ğŸ“– Demo 2: Zhipu Protocol (æ™ºè°±AI)");
    println!("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€");
    
    if let Ok(api_key) = std::env::var("ZHIPU_API_KEY") {
        let config = ModelConfig {
            provider: ModelProvider::Zhipu,
            model_name: "glm-4".to_string(),
            api_key: Some(api_key),
            endpoint: None, // ä½¿ç”¨ llm-connector å†…ç½®çš„ Zhipu ç«¯ç‚¹
            max_tokens: 100,
            temperature: 0.7,
        };
        
        let model = LlmModel::from_config(config)?;
        
        println!("âœ“ åè®®: {}", model.protocol_name());
        println!("âœ“ æ¨¡å‹åç§°: {}", model.model_name());
        println!("âœ“ ä½¿ç”¨ä¸“ç”¨æ„é€ å‡½æ•°: LlmClient::zhipu()");
        
        // Zhipu æ”¯æŒæ¨¡å‹å‘ç°
        match model.fetch_available_models().await {
            Ok(models) => {
                println!("âœ“ å¯ç”¨æ¨¡å‹: {:?}", models);
            }
            Err(e) => println!("âš ï¸  è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {}", e),
        }
    } else {
        println!("âš ï¸  æœªè®¾ç½® ZHIPU_API_KEY ç¯å¢ƒå˜é‡ï¼Œè·³è¿‡æ­¤æ¼”ç¤º");
        println!("   æç¤º: Zhipu AI ç°åœ¨æœ‰ä¸“ç”¨çš„æ„é€ å‡½æ•° LlmClient::zhipu()");
    }
    
    println!();
    Ok(())
}

async fn demo_aliyun() -> Result<(), Box<dyn std::error::Error>> {
    println!("ğŸ“– Demo 3: Aliyun Protocol (é˜¿é‡Œäº‘ DashScope)");
    println!("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€");
    
    if let Ok(api_key) = std::env::var("ALIYUN_API_KEY") {
        let config = ModelConfig {
            provider: ModelProvider::Aliyun,
            model_name: "qwen-max".to_string(),
            api_key: Some(api_key),
            endpoint: None, // ä½¿ç”¨ llm-connector å†…ç½®çš„ Aliyun ç«¯ç‚¹
            max_tokens: 100,
            temperature: 0.7,
        };
        
        let model = LlmModel::from_config(config)?;
        
        println!("âœ“ åè®®: {}", model.protocol_name());
        println!("âœ“ æ¨¡å‹åç§°: {}", model.model_name());
        println!("âœ“ ä½¿ç”¨ä¸“ç”¨æ„é€ å‡½æ•°: LlmClient::aliyun()");
        
        // Aliyun ä¸æ”¯æŒæ¨¡å‹å‘ç°
        println!("âš ï¸  Aliyun åè®®ä¸æ”¯æŒè‡ªåŠ¨æ¨¡å‹å‘ç°");
        println!("   å¯ç”¨æ¨¡å‹: qwen-turbo, qwen-plus, qwen-max");
    } else {
        println!("âš ï¸  æœªè®¾ç½® ALIYUN_API_KEY ç¯å¢ƒå˜é‡ï¼Œè·³è¿‡æ­¤æ¼”ç¤º");
        println!("   æç¤º: Aliyun DashScope ç°åœ¨æœ‰ä¸“ç”¨çš„æ„é€ å‡½æ•° LlmClient::aliyun()");
    }
    
    println!();
    Ok(())
}

async fn demo_ollama() -> Result<(), Box<dyn std::error::Error>> {
    println!("ğŸ“– Demo 4: Ollama Protocol (æœ¬åœ°æ¨¡å‹)");
    println!("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€");
    
    let config = ModelConfig {
        provider: ModelProvider::Ollama,
        model_name: "llama3.2".to_string(),
        api_key: None, // Ollama ä¸éœ€è¦ API key
        endpoint: None, // é»˜è®¤ http://localhost:11434
        max_tokens: 100,
        temperature: 0.7,
    };
    
    let model = LlmModel::from_config(config)?;
    
    println!("âœ“ åè®®: {}", model.protocol_name());
    println!("âœ“ æ¨¡å‹åç§°: {}", model.model_name());
    println!("âœ“ æ— éœ€ API key");
    
    // Ollama æ”¯æŒå®Œæ•´çš„æ¨¡å‹å‘ç°
    match model.fetch_available_models().await {
        Ok(models) => {
            println!("âœ“ æœ¬åœ°å·²å®‰è£…çš„æ¨¡å‹:");
            for (i, model_name) in models.iter().enumerate() {
                println!("  {}. {}", i + 1, model_name);
            }
            
            if models.is_empty() {
                println!("  (æœªæ‰¾åˆ°å·²å®‰è£…çš„æ¨¡å‹)");
                println!("  æç¤º: ä½¿ç”¨ 'ollama pull llama3.2' ä¸‹è½½æ¨¡å‹");
            }
        }
        Err(e) => {
            println!("âš ï¸  æ— æ³•è¿æ¥åˆ° Ollama: {}", e);
            println!("  æç¤º: ç¡®ä¿ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œ (http://localhost:11434)");
        }
    }
    
    println!();
    Ok(())
}

async fn demo_openai_compatible() -> Result<(), Box<dyn std::error::Error>> {
    println!("ğŸ“– Demo 5: OpenAI-Compatible Providers");
    println!("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€");
    
    // DeepSeek
    if let Ok(api_key) = std::env::var("DEEPSEEK_API_KEY") {
        println!("\nğŸ”¹ DeepSeek:");
        let config = ModelConfig {
            provider: ModelProvider::DeepSeek,
            model_name: "deepseek-chat".to_string(),
            api_key: Some(api_key),
            endpoint: None,
            max_tokens: 100,
            temperature: 0.7,
        };
        
        let model = LlmModel::from_config(config)?;
        println!("  åè®®: {}", model.protocol_name());
        
        match model.fetch_available_models().await {
            Ok(models) => println!("  å¯ç”¨æ¨¡å‹: {:?}", models),
            Err(e) => println!("  è·å–å¤±è´¥: {}", e),
        }
    }
    
    // Moonshot
    if let Ok(api_key) = std::env::var("MOONSHOT_API_KEY") {
        println!("\nğŸ”¹ Moonshot (Kimi):");
        let config = ModelConfig {
            provider: ModelProvider::Moonshot,
            model_name: "moonshot-v1-32k".to_string(),
            api_key: Some(api_key),
            endpoint: None,
            max_tokens: 100,
            temperature: 0.7,
        };
        
        let model = LlmModel::from_config(config)?;
        println!("  åè®®: {}", model.protocol_name());
        
        match model.fetch_available_models().await {
            Ok(models) => println!("  å¯ç”¨æ¨¡å‹: {:?}", models),
            Err(e) => println!("  è·å–å¤±è´¥: {}", e),
        }
    }
    
    // LongCat
    if let Ok(api_key) = std::env::var("LONGCAT_API_KEY") {
        println!("\nğŸ”¹ LongCat:");
        let config = ModelConfig {
            provider: ModelProvider::LongCat,
            model_name: "LongCat-Flash-Chat".to_string(),
            api_key: Some(api_key),
            endpoint: None,
            max_tokens: 100,
            temperature: 0.7,
        };
        
        let model = LlmModel::from_config(config)?;
        println!("  åè®®: {}", model.protocol_name());
        
        match model.fetch_available_models().await {
            Ok(models) => println!("  å¯ç”¨æ¨¡å‹: {:?}", models),
            Err(e) => println!("  è·å–å¤±è´¥: {}", e),
        }
    }
    
    if std::env::var("DEEPSEEK_API_KEY").is_err() 
        && std::env::var("MOONSHOT_API_KEY").is_err()
        && std::env::var("LONGCAT_API_KEY").is_err() {
        println!("\nâš ï¸  æœªè®¾ç½®ä»»ä½• OpenAI-compatible æä¾›å•†çš„ API key");
        println!("   æ”¯æŒçš„æä¾›å•†: DeepSeek, Moonshot, LongCat, VolcEngine ç­‰");
    }
    
    println!();
    Ok(())
}
